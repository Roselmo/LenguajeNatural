{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Importación de librerías\n",
        "\n",
        "- Se importan los módulos de torch para redes neuronales y entrenamiento\n",
        "- dataset se usa para cargar conjunto de datos\n",
        "- transformes facilita el uso de modelos NLP\n",
        "- skelearn.metrics se usa para evaluar el rendimiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oFJpS28J5Vuy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
        "from transformers import PreTrainedTokenizerFast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Carga del conjunto de datos\n",
        "\n",
        "- AG_NEWS es un dataset de clasificación de texto\n",
        "- se prepara el entrenamiento (dataset_train) y prueba (dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t08RrYP55Vu4",
        "outputId": "17aeb5a0-3fb6-4517-ea7d-56e47059f2e8"
      },
      "outputs": [],
      "source": [
        "# Load AG_NEWS dataset using the datasets library\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "dataset_train = dataset[\"train\"]\n",
        "dataset_test = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXIBLup--FVZ",
        "outputId": "6a009317-fcab-48a8-fbb9-4e4ac62847b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "120000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creación de un Tokenizador BPE\n",
        "\n",
        "- Se utiliza tokenizers para definir un tokenizador basado en Byte Pair Encoding (BPE)\n",
        "- Se configura para dividir texto en palabras usando espacios en blanco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize a BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "# Set the pre-tokenizer to split on whitespace\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrenamiento del tokenizador\n",
        "\n",
        "- Se define un entrenador BPE con un vocabulario de 1000 tokens\n",
        "- Seagregan tokens especiales como [PAD], [MASK], etc.\n",
        "- Se entrena el tokenizador en lotes (batch_iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nKOys8q5Vu7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Set up the trainer with special tokens\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=1000,\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "# Define a generator to yield batches of text\n",
        "def batch_iterator(batch_size=1000):\n",
        "    for i in range(0, len(dataset_train), batch_size):\n",
        "        yield dataset_train[i:i + batch_size][\"text\"]\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-Procesamiento y guardado del tokenizador \n",
        "\n",
        "- Usa *processors.TemplateProcessing* para definir cómo se manejan las secuencias individuales y pares de secuencias\n",
        "- Se configuran reglas de procesamiento de texto con tokens [CLS] y [SEP]\n",
        "- Se habilita truncamiento y padding hasta 128 tokens.\n",
        "- *tokenizer.enable_padding(...)* asegura que todas las secuencias tengan exactamente 128 tokens, rellenando con [PAD] si es necesario\n",
        "- Se guarda el tokenizador en un archivo JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the post-processor to add special tokens\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")\n",
        "# Enable truncation and padding\n",
        "tokenizer.enable_truncation(max_length=128)\n",
        "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\", length=128)\n",
        "\n",
        "tokenizer.save(\"custom_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cagado del tokenizador guardado\n",
        "\n",
        "- usa *PreTrainedTokenizerFast* para cargar el tokenizador desde el archivo *custom_tekenizer.json*\n",
        "- Esto permite reutilizar el tokenizador sin necesidad de configurarlo nuevamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RCx67gNs5VvD"
      },
      "outputs": [],
      "source": [
        "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Definición de la función *preprocess_data*\n",
        "\n",
        "- Esta función convierte textos en representaciones numéricas para que puedan ser utilizadas por un modelo de ML\n",
        "- Toma una lista de textos (*example[\"text\"]*) y la tokeniza en batch\n",
        "- *encode_batch* convierte cada texto en una secuencia de tokens según las reglas del tokenizador\n",
        "- *inputs_ids* son los IDs de los tokens generados por el tokenizador\n",
        "- *attention_mask* indica qué posiciones en la secuencia deben ser atendidas por el modelo (1=token, 0=padding)\n",
        "- Devuelve un diccionario con los *inputs_ids*, *attention_mask* y las etiquetas originales *(examples[\"label])*\n",
        "- Esto permite que los datos sean utilizados directamente en el entrenamiento de un modelo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6mbp8mWx5VvG"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(examples):\n",
        "    # Tokenize the text\n",
        "    encodings = tokenizer.encode_batch(examples[\"text\"])\n",
        "\n",
        "    # Extract input IDs and attention masks\n",
        "    input_ids = [encoding.ids for encoding in encodings]\n",
        "    attention_masks = [encoding.attention_mask for encoding in encodings]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_masks,\n",
        "        \"label\": examples[\"label\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aplicación de la función *preprocess_data*\n",
        "\n",
        "- Se aplica la función al conjunto de datos de entrenamiento (*dataset_train*) y prueba (*dataset_test*)\n",
        "- Usa *map* para aplicar *preprocess_data* a cada ejemplo en *dataset_train* y *dataset_test*\n",
        "- *batched=True* significa que la función recibe múltiples ejemplos a la vez, lo que mejora la eficiencia.\n",
        "- L función *preprocess_data* convierte el texto en *input_ids* y *attention_mask*\n",
        "*map* almacena los resultados en un nuevo conjunto de datos (*ds_train* y *ds_test*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "0d901d85b12f4cfb97680df26dd8a4b5",
            "6f4da76254b54d2a867536e5fe94f0f2",
            "7a9afdff75834370b61278b9553e236d",
            "269e7541a51f4a1384694d4d909c73c3",
            "b043759f01904768b6aeb3bfa57576a2",
            "c8367d3f923d4a28b3cab59e4a232ef9",
            "1d702235597f4140ae15907b108fc16b",
            "b7acd48492004dc78a811d4a0d7099d8",
            "9ccf9a90cf604becb029f74f1c1c3b5c",
            "0c5ee534f9cd4fa29d5c24cca67b189d",
            "808b802749254776ae3a1c6a3271283c",
            "179a570fbfc2422fa1856f77b38f41b4",
            "bb5546c03ccc4a7ba1aec4d8ef0035b3",
            "783ff803cb454ea38ebd5f729862e873",
            "5fb8c078c70a42168a86f0cba7ccd11f",
            "02524b7dd9304c51b5d0895da261817d",
            "8c6bc2bf7b3641828801e2b5d695ed61",
            "3940058740b84b2c93b43d6b59e52d20",
            "967059020aa34e6985a6edd21a4bd84d",
            "29e82579ccf447a8bb2bd10969512c16",
            "0a1af3151f69403989197f7091432aef",
            "0dc5ff69923d4821a9fbe56425669f30"
          ]
        },
        "id": "DRDBWjHL5VvK",
        "outputId": "ab2bee88-154b-4218-fd17-af69047b0ac2"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing\n",
        "ds_train = dataset_train.map(preprocess_data, batched=True)\n",
        "ds_test = dataset_test.map(preprocess_data, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Este diccionarioo muestra como se transformó el texto original en una representación numérica para el modelo. \n",
        "- Elementos clave:\n",
        "    - *text*: Contiene la frase original.\n",
        "    - *label*: La categoría asignada al texto (en este caso, 2).\n",
        "    - *input_ids*:Es la versión tokenizada del texto, donde cada palabra (o subpalabra) ha sido convertida en un número entero. El primer número 1 representa el token [CLS], que indica el inicio de la secuencia. Los ceros al final son padding para alcanzar la longitud máxima (128).\n",
        "    - *attention_mask*: Indica qué tokens son relevantes (1) y cuáles son relleno (0). Los primeros valores 1 corresponden a los tokens reales, mientras que los ceros al final ignoran el padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VIbBkf-5VvO",
        "outputId": "ee8132a9-c6bb-4022-ff6f-d8a1bbeb66cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
              " 'label': 2,\n",
              " 'input_ids': [1,\n",
              "  54,\n",
              "  150,\n",
              "  202,\n",
              "  16,\n",
              "  33,\n",
              "  996,\n",
              "  34,\n",
              "  699,\n",
              "  33,\n",
              "  215,\n",
              "  157,\n",
              "  99,\n",
              "  95,\n",
              "  923,\n",
              "  215,\n",
              "  11,\n",
              "  219,\n",
              "  12,\n",
              "  219,\n",
              "  15,\n",
              "  383,\n",
              "  692,\n",
              "  15,\n",
              "  78,\n",
              "  245,\n",
              "  133,\n",
              "  14,\n",
              "  54,\n",
              "  150,\n",
              "  202,\n",
              "  104,\n",
              "  124,\n",
              "  10,\n",
              "  78,\n",
              "  63,\n",
              "  82,\n",
              "  286,\n",
              "  71,\n",
              "  101,\n",
              "  58,\n",
              "  61,\n",
              "  112,\n",
              "  103,\n",
              "  745,\n",
              "  144,\n",
              "  15,\n",
              "  62,\n",
              "  84,\n",
              "  73,\n",
              "  605,\n",
              "  14,\n",
              "  192,\n",
              "  151,\n",
              "  64,\n",
              "  101,\n",
              "  66,\n",
              "  104,\n",
              "  96,\n",
              "  362,\n",
              "  16,\n",
              "  2,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3,\n",
              "  3],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo básico de embedding para claseificar textos\n",
        "\n",
        "- Es una red neuronal simple con una capa de embeddings y una capa lineal\n",
        "- Definición de la clase *BasicEmbeddingModel*: Hereda de nn.Module, lo que significa que es un modelo de PyTorch.\n",
        "- *__init__ (Inicialización del modelo)*:\n",
        "\t- *vocab_size*: Tamaño del vocabulario (cantidad de palabras únicas que el modelo puede manejar).\n",
        "\t- *embed_dim*: Dimensión de los embeddings (representación vectorial de cada palabra).\n",
        "\t- *num_classes*: Número de categorías para la clasificación.\n",
        "\t- *self.embedding* = nn.Embedding(vocab_size, embed_dim, padding_idx=3). Capa de embeddings que transforma los input_ids (tokens) en vectores. *padding_idx=3* hace que los tokens de padding no contribuyan al aprendizaje.\n",
        "\t- *self.fc* = nn.Linear(embed_dim, num_classes). Capa lineal que toma el embedding final y lo proyecta al número de clases.\n",
        "- *forward (Propagación hacia adelante)*:\n",
        "    - *embedded = self.embedding(input_ids)* Convierte los tokens en vectores de embedding.\n",
        "\t- *embedded = torch.mean(embedded, dim=1)* Promedia los embeddings a lo largo de la dimensión de las palabras (pooling promedio).\n",
        "\t- *logits = self.fc(embedded)* Pasa el embedding resultante por la capa lineal para obtener las predicciones.\n",
        "- Cálculo de la pérdida (opcional):\n",
        "\t- Se usa *nn.CrossEntropyLoss()* para calcular la pérdida entre logits y las etiquetas reales.\n",
        "\t- Se devuelve solo la salida (logits), que contiene las predicciones del modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QTUQ9uXk5VvP"
      },
      "outputs": [],
      "source": [
        "# Define Basic Embedding Model\n",
        "class BasicEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=3)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = torch.mean(embedded, dim=1)  # Averaging embeddings\n",
        "        logits = self.fc(embedded)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configuración del modelo de embeddings\n",
        "\n",
        "- Definir hiperparámetros del modelo\n",
        "\t- *embed_dim = 100*: Cada palabra será representada por un vector de 100 dimensiones.\n",
        "\t- *num_classes = 4*: El modelo clasificará el texto en 4 categorías posibles.\n",
        "\t- *vocab_size = 1000*: El modelo tiene un vocabulario de 1000 palabras únicas.\n",
        "- Se crea un modelo *BasicEmbeddingModel* con los parámetros anteriores\n",
        "- Se imprime la arquitectura del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Arquitectura del modelo:_\n",
        "\n",
        "*Embedding(1000, 100, padding_idx=3)*\n",
        "\n",
        "    - Es la capa de embeddings que toma tokens (números) y los convierte en vectores de tamaño 100.\n",
        "    - 1000 es el tamaño del vocabulario (cantidad de palabras únicas que el modelo reconoce).\n",
        "    - *padding_idx=3* significa que el índice 3 corresponde al token de padding, que no contribuirá al entrenamiento.\n",
        "    \n",
        "*Linear(in_features=100, out_features=4, bias=True)*\n",
        "\n",
        "    - Es la capa lineal de clasificación.\n",
        "    - *in_features=100*: Recibe un vector de 100 dimensiones (el embedding promediado de las palabras).\n",
        "    - *out_features=4*: Produce 4 valores de salida (uno para cada clase de clasificación).\n",
        "    - *bias=True*: Incluye un término de sesgo en la capa lineal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F-OArCo5VvQ",
        "outputId": "82a5a9cb-0589-4d2b-835d-4aac09380f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BasicEmbeddingModel(\n",
            "  (embedding): Embedding(1000, 100, padding_idx=3)\n",
            "  (fc): Linear(in_features=100, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Model setup\n",
        "embed_dim = 100\n",
        "num_classes = 4\n",
        "vocab_size = 1000\n",
        "model = BasicEmbeddingModel(vocab_size, embed_dim, num_classes)\n",
        "# Show model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Métricas de evaluación para el modelo \n",
        "\n",
        "- Recibe eval_pred, que es una tupla (logits, labels), donde:\n",
        "    - logits: Son las salidas del modelo antes de aplicar softmax.\n",
        "    - labels: Son las etiquetas reales del conjunto de prueba.\n",
        "    - np.argmax(logits, axis=1): Toma el índice del valor más alto en cada fila de logits, que corresponde a la clase predicha.\n",
        "    - accuracy_score(labels, predictions): Compara las predicciones con las etiquetas verdaderas y calcula el porcentaje de aciertos.\n",
        "- f1_score(labels, predictions, average='weighted'): Calcula el F1-score considerando el equilibrio entre precisión y exhaustividad (precision y recall). La opción 'weighted' pondera cada clase según su frecuencia en el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xDLNtzT05VvR"
      },
      "outputs": [],
      "source": [
        "# Define compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuración los argumentos de entrenamiento para el modelo usando la clase TrainingArguments de la biblioteca transformers. Define aspectos clave del proceso de entrenamiento, como el directorio de salida, la frecuencia de evaluación y guardado, la tasa de aprendizaje y el tamaño del lote.\n",
        "\n",
        "*Tamaño del lote (batch_size):*\n",
        "- 32 ejemplos por dispositivo durante el entrenamiento.\n",
        "- 32 ejemplos por dispositivo durante la evaluación.\n",
        "- Si se usa una GPU, cada GPU procesará un lote de 32 ejemplos.\n",
        "\n",
        "*Número de épocas:* El modelo pasará tres veces por el conjunto de datos de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1QbhdUZ15VvS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este código crea un objeto Trainer, una clase de la biblioteca transformers que automatiza el entrenamiento y evaluación de modelos de deep learning.\n",
        "\n",
        "- Se crea una instancia de Trainer, que maneja el entrenamiento y evaluación del modelo.\n",
        "- Modelo a entrenar, en este caso, model, que es una instancia de BasicEmbeddingModel\n",
        "- Argumentos de entrenamiento, previamente definidos en training_args (como tasa de aprendizaje, número de épocas, etc.).\n",
        "- Conjuntos de datos: *ds_train* (Datos de entrenamiento) y  *ds_test* (Datos de evaluación).\n",
        "- Tokenizador: fast_tokenizer es el tokenizador preentrenado que se usará para procesar los datos antes de alimentarlos al modelo.\n",
        "- Función de métricas: Usa la función compute_metrics, que calcula precisión (accuracy) y puntuación F1 (f1-score) en cada evaluación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZfcmL4tv6ml_"
      },
      "outputs": [],
      "source": [
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_test,\n",
        "    processing_class=fast_tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "6_D8pr-46tNk",
        "outputId": "0dc479f3-6a8d-4033-b3e9-f8f636a44566"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11250/11250 01:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.483400</td>\n",
              "      <td>0.483327</td>\n",
              "      <td>0.831711</td>\n",
              "      <td>0.831137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.432300</td>\n",
              "      <td>0.446807</td>\n",
              "      <td>0.843816</td>\n",
              "      <td>0.843414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.425600</td>\n",
              "      <td>0.440676</td>\n",
              "      <td>0.845000</td>\n",
              "      <td>0.844646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=11250, training_loss=0.5115090135362413, metrics={'train_runtime': 64.1468, 'train_samples_per_second': 5612.125, 'train_steps_per_second': 175.379, 'total_flos': 0.0, 'train_loss': 0.5115090135362413, 'epoch': 3.0})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El entrenamiento del modelo mostró una mejora progresiva en su desempeño a lo largo de tres épocas. La pérdida de entrenamiento disminuyó de 0.4834 en la primera época a 0.4256 en la tercera, indicando que el modelo ajustó mejor los datos con cada iteración. De manera similar, la pérdida de validación se redujo de 0.4833 a 0.4407, lo que sugiere una mejora en la capacidad de generalización del modelo. En términos de precisión, el modelo pasó de un 83.17 % en la primera época a un 84.50 % en la última, mientras que el puntaje F1, que mide el equilibrio entre precisión y sensibilidad, aumentó de 83.11 % a 84.46 %. Estos resultados reflejan un aprendizaje estable y una mejora en la capacidad del modelo para realizar predicciones precisas sin evidencias de sobreajuste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JfYA3yH3ej2"
      },
      "source": [
        "# Lab: Enhancing a Basic Embedding Model with Positional Encoding and Multi-Head Attention\n",
        "\n",
        "## Objective\n",
        "In this lab, you will modify a basic sentiment analysis model by adding two key components from transformer-based architectures: **positional encoding** and **multi-head attention**. These modifications will improve the model’s ability to capture word order and token interactions, making it more expressive.\n",
        "\n",
        "---\n",
        "\n",
        "## Task Overview\n",
        "You are provided with a basic embedding model that performs sentiment analysis by:\n",
        "- Mapping token IDs to embeddings.\n",
        "- Averaging embeddings across tokens.\n",
        "- Passing the result through a fully connected layer for classification.\n",
        "\n",
        "Your tasks:\n",
        "1. **Implement positional encoding** to enrich word embeddings with information about token positions.\n",
        "2. **Replace the mean pooling operation** with a **multi-head self-attention layer** to allow tokens to attend to each other.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Implement Positional Encoding\n",
        "Transformers lack recurrence, so they rely on positional encodings to incorporate token order. You will implement **sinusoidal positional encoding**, as described in Vaswani et al. (2017), and add it to the input embeddings.\n",
        "\n",
        "### Requirements\n",
        "- Implement a function to generate sinusoidal positional encodings.\n",
        "- Ensure the encoding is applied correctly to all input embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2: Implement Multi-Head Self-Attention\n",
        "Instead of simply averaging word embeddings, you will replace this operation with **multi-head self-attention** to allow interactions between tokens.\n",
        "\n",
        "### Requirements\n",
        "- Implement a multi-head self-attention layer.\n",
        "- Replace the `torch.mean(embedded, dim=1)` operation with the **attention mechanism**.\n",
        "- Ensure that the implementation correctly processes padded tokens.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4mCov8P4cJT"
      },
      "source": [
        "## Starter Code\n",
        "Below is the base implementation of the model. You must complete the missing parts and train the model.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=3)\n",
        "        \n",
        "        # Task 1: Implement positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
        "\n",
        "        # Task 2: Implement Multi-Head Attention (replace mean pooling)\n",
        "        # self.multihead_attn = ???\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        \n",
        "        # Apply positional encoding\n",
        "        embedded = self.positional_encoding(embedded)\n",
        "\n",
        "        # Task: Replace mean pooling with Multi-Head Attention\n",
        "        # attn_output, _ = ???(embedded, embedded, embedded, key_padding_mask=attention_mask)\n",
        "        \n",
        "        # Pooling: Extract the first token's representation (CLS token equivalent)\n",
        "        # pooled_output = attn_output[:, 0, :]\n",
        "\n",
        "        logits = self.fc(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}\n",
        "\n",
        "# Students must implement this class\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.encoding = self.create_positional_encoding(embed_dim, max_len)\n",
        "        \n",
        "    def create_positional_encoding(self, embed_dim, max_len):\n",
        "        # Task: Implement sinusoidal positional encoding here\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Task: Add positional encoding to embeddings\n",
        "        pass\n",
        "```\n",
        "\n",
        "## Evaluation Criteria\n",
        "\n",
        "To verify your implementation:\n",
        "\n",
        "- Ensure that positional encoding correctly modifies embeddings.\n",
        "\n",
        "- Confirm that the self-attention layer replaces mean pooling effectively.\n",
        "\n",
        "- Train the modified model on a small sentiment dataset and compare results.\n",
        "\n",
        "## Extra Challenges\n",
        "\n",
        "For advanced students:\n",
        "\n",
        "- Experiment with different pooling strategies (e.g., max pooling, CLS token extraction).\n",
        "\n",
        "- Compare learned positional embeddings vs. sinusoidal encodings.\n",
        "\n",
        "- Add layer normalization after attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tarea 1: Implementación de Positional Encoding \n",
        "\n",
        "Introduce una representación posicional en los embeddings para que el modelo pueda capturar información sobre el orden de las palabras en una secuencia. A diferencia de los modelos tradicionales que dependen de estructuras recurrentes (como RNNs), los modelos basados en atención, como Transformers, no tienen una noción inherente de orden, por lo que se debe agregar manualmente esta información a través de codificaciones posicionales.\n",
        "\n",
        "1.\tInicialización de la clase\n",
        "- *PositionalEncoding(nn.Module)*: Se define la clase como una subclase de nn.Module.\n",
        "- *embed_dim*: Dimensión del espacio de embedding.\n",
        "- *max_len=5000*: Longitud máxima de la secuencia a considerar.\n",
        "2.\tCreación de la matriz de codificación posicional (create_positional_encoding)\n",
        "- *position = torch.arange(max_len).unsqueeze(1)*: Crea un tensor con las posiciones de cada token en la secuencia, expandido a una dimensión extra para operaciones posteriores.\n",
        "*div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))*: Calcula los factores de escala para las funciones seno y coseno, siguiendo la fórmula estándar de codificación sinusoidal.\n",
        "- *pe = torch.zeros(max_len, embed_dim)*: Inicializa la matriz de codificación posicional con ceros.\n",
        "- *pe[:, 0::2] = torch.sin(position * div_term)*: Aplica la función seno a las posiciones de índice par.\n",
        "- *pe[:, 1::2] = torch.cos(position * div_term)*: Aplica la función coseno a las posiciones de índice impar.\n",
        "- *return pe.unsqueeze(0)*: Agrega una dimensión para que coincida con la forma (1, max_len, embed_dim) y pueda sumarse fácilmente a los embeddings.\n",
        "3.\tPropagación (forward)\n",
        "- *x + self.encoding[:, :x.size(1)]*: Se suma la codificación posicional a los embeddings de entrada, asegurando que la secuencia tenga la misma longitud que la entrada.\n",
        "\n",
        "El propósito de esta implementación es mejorar la capacidad del modelo para distinguir la posición de cada palabra en una oración, lo que es crucial en tareas de procesamiento de lenguaje natural. La combinación de senos y cosenos permite que la representación posicional generalice mejor a longitudes de secuencia que el modelo no ha visto durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Task 1: Implement Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.encoding = self.create_positional_encoding(embed_dim, max_len)\n",
        "        \n",
        "    def create_positional_encoding(self, embed_dim, max_len):\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tarea 2: Implementación de Multi-Head Attention (Reemplazo del Mean Pooling)\n",
        "\n",
        "Esta tarea introduce la Atención Multi-Cabeza (Multi-Head Attention, MHA) en lugar de la estrategia de Mean Pooling utilizada previamente. La MHA permite que el modelo capture relaciones complejas entre palabras en la secuencia, proporcionando una representación más rica y contextualizada.\n",
        "\n",
        "*Explicación del código*:\n",
        "\n",
        "Inicialización del modelo (__init__)\n",
        "- *embed_dim*: Dimensión del embedding para cada palabra.\n",
        "- *num_heads*: Número de cabezas de atención en la capa Multi-Head Attention.\n",
        "- *vocab_size*: Número total de palabras en el vocabulario.\n",
        "- *max_len=5000*: Longitud máxima de las secuencias para la codificación posicional.\n",
        "\n",
        "Componentes principales\n",
        "1. *Embedding Layer (self.embedding).* Convierte los índices de las palabras en vectores densos de embed_dim dimensiones.\n",
        "2. *Positional Encoding (self.pos_encoding).* Se añade la información de la posición de las palabras en la secuencia, para que el modelo pueda distinguir el orden.\n",
        "3. *Multi-Head Attention (self.multihead_attn).* Implementa la capa de Atención Multi-Cabeza para capturar relaciones a largo plazo dentro de la secuencia. *nn.MultiheadAttention(embed_dim, num_heads)*, permite que el modelo aprenda diferentes representaciones de atención en paralelo.\n",
        "4. *Layer Normalization (self.norm)*. (Extra Challenge), la normalización ayuda a estabilizar el entrenamiento al mantener valores dentro de un rango bien definido.\n",
        "5. *Capa de salida (self.fc)*. Es una capa completamente conectada (*Linear(embed_dim, 1)*) que reduce la representación final a una salida para clasificación.\n",
        "6. Propagación hacia adelante (forward)\n",
        "\t- *embedded = self.embedding(x)*: Convierte los tokens en vectores densos de embed_dim dimensiones. Salida (batch_size, seq_len, embed_dim).\n",
        "\t- *embedded = self.pos_encoding(embedded)*: Se suma la codificación posicional para agregar información sobre el orden de la secuencia.\n",
        "\t- *embedded = embedded.permute(1, 0, 2)*: Reorganiza las dimensiones para que coincida con la entrada esperada por nn.MultiheadAttention. Nueva forma (seq_len, batch_size, embed_dim). Esto es necesario porque PyTorch implementa nn.MultiheadAttention esperando el formato (longitud de secuencia, tamaño del lote, dimensiones de embedding).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Implement Multi-Head Attention (Replace Mean Pooling)\n",
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, vocab_size, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim, max_len)\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, embed_dim)\n",
        "        embedded = self.pos_encoding(embedded)  # Apply positional encoding\n",
        "        embedded = embedded.permute(1, 0, 2)  # Change shape for MultiheadAttention (seq_len, batch_size, embed_dim)\n",
        "                \n",
        "# Task 3: Replace Mean Pooling with Multi-Head Attention\n",
        "\n",
        "#En esta tarea, se reemplaza la estrategia de Mean Pooling por Atención Multi-Cabeza \n",
        "# para mejorar la capacidad del modelo de capturar relaciones contextuales dentro de \n",
        "# la secuencia. La línea attn_output, _ = \n",
        "# self.multihead_attn(embedded, embedded, embedded, key_padding_mask=attention_mask) \n",
        "# aplica la Autoatención Multi-Cabeza, donde cada palabra en la secuencia puede atender \n",
        "# a otras palabras, permitiendo que el modelo aprenda representaciones más ricas y \n",
        "# contextualmente informadas. Posteriormente, attn_output = attn_output.permute(1, 0, 2) \n",
        "# reorganiza las dimensiones para restaurar el formato esperado (batch_size, seq_len, embed_dim). \n",
        "# Finalmente, se aplica Normalización por Capas (self.norm(attn_output)) para estabilizar \n",
        "# el entrenamiento y mejorar la convergencia del modelo. Este enfoque optimiza la representación \n",
        "# de las secuencias, mejorando la capacidad del modelo para comprender la estructura del \n",
        "# lenguaje.\n",
        "\n",
        "        attn_output, _ = self.multihead_attn(embedded, embedded, embedded, key_padding_mask=attention_mask)\n",
        "        attn_output = attn_output.permute(1, 0, 2)  # Back to (batch_size, seq_len, embed_dim)\n",
        "        attn_output = self.norm(attn_output)  # Layer normalization (Extra Challenge)\n",
        "        \n",
        "# Task 4: Extract First Token Representation (CLS Token Equivalent)\n",
        "\n",
        "# En esta tarea, se extrae la representación del primer token de la secuencia \n",
        "# (CLS token equivalent), lo cual es una técnica común en modelos de procesamiento de \n",
        "# lenguaje natural para capturar una representación global del texto. \n",
        "# La operación pooled_output = attn_output[:, 0, :] selecciona únicamente la salida \n",
        "# correspondiente al primer token de la secuencia, que contiene información agregada a \n",
        "# partir del mecanismo de atención. Finalmente, esta representación pasa a través de \n",
        "# una capa totalmente conectada (self.fc(pooled_output)) y se aplica la función sigmoide \n",
        "# (torch.sigmoid) para obtener una probabilidad en una tarea de clasificación binaria. \n",
        "# Este enfoque permite que el modelo aprenda una representación compacta y efectiva del \n",
        "# texto de entrada.\n",
        "\n",
        "        pooled_output = attn_output[:, 0, :]  # Take the first token output\n",
        "        \n",
        "        return torch.sigmoid(self.fc(pooled_output))  # Binary classification output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Output Shape: torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "# Extra Challenges:\n",
        "# 1. Experimenting with different pooling strategies\n",
        "\n",
        "# En este desafío adicional, se implementa una variante del modelo que reemplaza la \n",
        "# estrategia de extracción del primer token (CLS token equivalent) por una técnica de \n",
        "# max pooling. En lugar de seleccionar únicamente la representación del primer token, \n",
        "# la operación torch.max(attn_output, dim=1) extrae el valor máximo a lo largo de la \n",
        "# dimensión de secuencia, lo que permite capturar la característica más relevante de \n",
        "# cada dimensión del espacio latente. Este enfoque puede ser útil en tareas donde la \n",
        "# información más importante no siempre se concentra en la primera posición de la secuencia. \n",
        "# Finalmente, la salida pasa por una capa totalmente conectada seguida de una función sigmoide\n",
        "# para la clasificación binaria.\n",
        "class MaxPoolingModel(AttentionModel):\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.pos_encoding(embedded)\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        attn_output, _ = self.multihead_attn(embedded, embedded, embedded, key_padding_mask=attention_mask)\n",
        "        attn_output = attn_output.permute(1, 0, 2)\n",
        "        attn_output = self.norm(attn_output)\n",
        "        pooled_output, _ = torch.max(attn_output, dim=1)  # Max pooling instead of CLS token\n",
        "        return torch.sigmoid(self.fc(pooled_output))\n",
        "\n",
        "# 2. Comparing learned positional embeddings vs. sinusoidal encodings\n",
        "\n",
        "# n esta implementación, se compara el uso de codificaciones posicionales aprendidas con \n",
        "# las codificaciones sinusoidales predefinidas. A diferencia del método basado en funciones \n",
        "# trigonométricas, aquí se utiliza una matriz de parámetros entrenable (nn.Parameter), \n",
        "# lo que permite que el modelo aprenda una representación posicional óptima a partir de \n",
        "# los datos. Durante la propagación, la codificación aprendida se suma a las representaciones \n",
        "# de los tokens de entrada, similar a la versión sinusoidal. Posteriormente, se integra en \n",
        "# un modelo de Multi-Head Attention para evaluar su desempeño. Esta comparación es útil para \n",
        "# determinar si una codificación fija es suficiente o si el modelo se beneficia de aprender \n",
        "# sus propias posiciones.\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.encoding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)]  # Learned encoding instead of sinusoidal\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = 10000\n",
        "    embed_dim = 128\n",
        "    num_heads = 8\n",
        "    seq_len = 50\n",
        "    batch_size = 32\n",
        "    \n",
        "    model = AttentionModel(embed_dim, num_heads, vocab_size)\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "    attention_mask = torch.zeros(batch_size, seq_len).bool()  # No padding in this case\n",
        "    output = model(input_ids, attention_mask)\n",
        "    print(\"Model Output Shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learned Positional Encoding: torch.Size([1, 5000, 128])\n"
          ]
        }
      ],
      "source": [
        "# Valores de la codificación posicional aprendida\n",
        "print(\"Learned Positional Encoding:\", model.pos_encoding.encoding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: torch.Size([32, 50])\n",
            "Output Shape: torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "# Dimensiones de entrada y salida\n",
        "print(\"Input Shape:\", input_ids.shape)\n",
        "print(\"Output Shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Embedding Shape: torch.Size([32, 50, 128])\n",
            "Sample Embedding Values: tensor([[-6.3791e-01, -3.0183e-01,  1.0202e-01,  7.6067e-01, -8.1699e-01,\n",
            "         -1.4822e+00,  4.2339e-01,  3.9453e-01, -2.6421e-01, -8.8591e-01,\n",
            "         -5.4635e-01, -1.3147e+00, -2.7177e+00,  4.0065e-01,  5.7290e-01,\n",
            "          3.9411e-01, -7.6752e-01,  5.8985e-01, -2.5876e-01,  1.3223e+00,\n",
            "         -4.1940e-01,  5.9033e-01,  6.6007e-01,  1.0046e+00, -2.6067e-01,\n",
            "         -3.6654e-01, -9.4022e-01,  1.6248e+00, -7.6404e-01,  5.7202e-01,\n",
            "         -1.3360e+00, -5.2518e-01, -5.3944e-01,  8.3092e-01, -5.8592e-02,\n",
            "         -2.5962e-01, -1.3398e+00,  1.0622e+00, -7.0613e-01,  1.1874e+00,\n",
            "          8.2952e-01, -1.0658e+00,  2.4963e+00,  1.5158e+00, -7.7159e-01,\n",
            "          7.5991e-01, -1.5570e+00, -7.7714e-01, -8.5059e-01,  8.0556e-01,\n",
            "         -1.7259e+00,  4.3618e-01, -1.1123e+00, -8.0159e-02, -1.2766e+00,\n",
            "         -1.9401e+00, -4.9817e-01,  9.2177e-01, -1.0612e+00,  1.6774e+00,\n",
            "          7.3943e-01,  2.0938e+00, -4.0729e-02, -2.4596e-03,  4.6353e-01,\n",
            "          5.0542e-01, -2.6016e-01,  1.3047e+00,  2.5760e+00,  9.4576e-01,\n",
            "         -5.8629e-01, -2.2262e+00,  3.3459e-01,  1.5450e+00,  5.6077e-01,\n",
            "          9.2208e-01, -1.8207e-01, -9.2250e-01, -8.7723e-01, -6.0990e-02,\n",
            "          2.0329e+00,  1.0992e+00,  8.7508e-01,  6.6120e-01, -1.1078e+00,\n",
            "          1.0828e+00,  1.9402e+00, -9.6309e-01, -3.8961e-01,  1.5987e+00,\n",
            "         -1.7223e-02, -9.4550e-01, -6.9610e-01, -6.8685e-01,  7.7520e-01,\n",
            "          5.8345e-01, -1.2488e+00, -6.3556e-01,  4.3281e-01,  1.3662e+00,\n",
            "          2.4234e-01, -5.9099e-01, -1.1604e+00,  1.0640e+00,  1.1499e+00,\n",
            "         -1.8670e-01,  9.6966e-01,  6.4354e-01, -1.0087e+00, -1.0103e+00,\n",
            "         -2.3431e+00, -9.2514e-01, -3.5443e-01, -2.9594e-01,  1.5623e+00,\n",
            "         -3.6139e-01,  9.1176e-01,  1.2590e+00,  1.4754e+00,  1.2996e+00,\n",
            "          8.4756e-01,  1.7285e-02,  1.1787e+00,  2.1120e-01, -2.4186e+00,\n",
            "          2.7339e-01,  7.6555e-01,  5.9155e-01],\n",
            "        [ 2.6914e-01,  4.4846e-01, -1.6785e-01,  2.2292e+00, -1.4870e+00,\n",
            "          4.8726e-02, -8.7939e-01, -1.9789e+00,  8.4460e-01,  1.0992e+00,\n",
            "         -2.5225e-01, -4.5773e-01,  1.3593e-01,  1.0589e+00, -1.0655e-02,\n",
            "          1.5429e-01, -7.6738e-01,  1.2501e+00,  1.7367e+00, -1.9936e-01,\n",
            "          1.7211e-01,  1.0276e+00, -6.6867e-01, -2.7940e+00, -6.7436e-01,\n",
            "          1.4702e+00,  7.2281e-01, -2.4347e+00,  1.0877e+00, -5.8003e-01,\n",
            "         -2.0273e-01,  1.2188e-01, -4.9470e-01,  7.1786e-01, -8.7253e-01,\n",
            "          1.3579e+00,  1.1707e+00, -6.6791e-01, -2.7324e-01,  1.5456e+00,\n",
            "         -9.9845e-01,  9.4538e-01, -3.4830e-01,  2.7702e-01,  1.5014e-01,\n",
            "         -5.8588e-01, -5.0178e-02, -4.4981e-01,  9.6090e-01, -3.5899e-01,\n",
            "          3.0952e-01, -1.4452e+00,  4.3387e-01, -1.3253e+00, -1.5906e+00,\n",
            "         -1.5470e-01,  9.7652e-01,  6.0881e-01,  2.1676e+00, -4.1166e-01,\n",
            "          2.9387e-01,  4.0448e-01,  4.6456e-01,  7.3510e-01,  9.0352e-01,\n",
            "          3.3755e-01,  4.7142e-01, -4.6981e-01,  7.4765e-01, -1.4148e+00,\n",
            "          5.1572e-01,  4.7614e-01,  1.4397e+00, -1.9173e+00, -2.3595e-01,\n",
            "          3.7192e-01, -1.0537e+00,  1.1558e+00,  2.5201e-02, -6.1710e-01,\n",
            "          3.4994e-01,  7.1283e-01, -4.2068e-01, -1.5328e-01, -2.6264e-01,\n",
            "         -8.4233e-02,  8.2077e-01,  1.2178e+00, -1.5733e-01, -4.5164e-01,\n",
            "         -1.3013e+00, -1.0607e+00, -1.8165e-01, -8.7203e-01, -7.9183e-01,\n",
            "         -1.7257e+00,  1.4593e-01,  1.1660e+00,  4.9621e-01,  4.1012e-01,\n",
            "          8.6588e-01,  2.1351e-02, -1.5787e-01, -1.3580e+00, -1.4520e+00,\n",
            "          8.9205e-01, -1.1208e+00,  9.2797e-01,  1.0218e-01, -1.0667e+00,\n",
            "         -1.0635e+00,  2.2689e-01, -1.3044e-01, -1.1805e+00,  8.1678e-01,\n",
            "         -2.1177e-01,  5.9081e-01,  3.5054e-01, -9.7919e-01,  1.8854e+00,\n",
            "          1.3569e+00,  1.3052e-01,  1.3532e+00,  1.7663e+00,  3.3599e-01,\n",
            "          1.3979e+00, -4.0456e-01, -2.0609e-01],\n",
            "        [-4.3681e-01, -2.5626e-01,  4.6553e-01, -2.7916e-01, -2.1884e+00,\n",
            "         -6.3947e-01,  2.7253e-01, -4.8339e-01,  2.1405e-02,  7.1632e-01,\n",
            "         -8.5315e-01, -1.3948e-01,  5.6670e-01,  1.7432e+00,  6.8350e-02,\n",
            "         -1.0731e+00, -9.1809e-02, -5.6132e-01,  3.3359e-01, -1.1940e+00,\n",
            "          1.7149e-01, -1.9893e+00,  3.8719e-01, -2.1441e+00, -1.9256e+00,\n",
            "         -1.2139e+00, -2.9613e-01,  1.7153e+00, -5.3892e-01,  1.6259e+00,\n",
            "          1.5753e-01,  1.6883e+00, -5.8187e-01, -3.3864e-01,  5.8113e-01,\n",
            "          5.3367e-01,  9.0941e-01,  3.1346e-01,  5.3980e-01, -1.5040e+00,\n",
            "          4.9476e-01, -4.9523e-01, -3.2681e-01, -9.3800e-01, -3.6393e-01,\n",
            "         -3.9430e-01,  5.3292e-01,  3.9457e-01,  1.3665e+00, -9.4220e-02,\n",
            "          6.4719e-01,  6.3214e-01,  1.6689e+00,  1.1321e+00,  7.6839e-01,\n",
            "          1.3199e+00, -9.5559e-03, -5.9789e-01, -1.6348e+00, -1.5292e-01,\n",
            "          1.4396e+00,  7.4666e-01,  9.9840e-01,  5.7940e-01, -6.5482e-02,\n",
            "          4.4065e-01,  6.1451e-01, -1.1036e-01,  5.4975e-01, -9.1167e-01,\n",
            "          1.2219e+00,  1.0501e+00, -6.0780e-01, -1.1999e+00,  5.8144e-01,\n",
            "         -1.3688e+00, -2.9834e-01, -4.6884e-01, -1.2699e+00,  4.3235e-02,\n",
            "          1.2219e+00,  8.3296e-01,  8.1149e-01,  2.2479e+00, -5.6400e-01,\n",
            "          2.2156e-01,  6.0582e-01,  1.0251e+00, -7.8636e-04,  9.9460e-01,\n",
            "         -8.8415e-01, -2.1809e-01, -9.7464e-02,  2.2075e-01, -4.8337e-01,\n",
            "          3.8688e-01,  3.1086e-01,  6.0701e-01,  5.3056e-01, -2.9951e-01,\n",
            "          1.1061e+00, -1.1695e+00,  5.9104e-01,  9.4184e-01, -7.4596e-01,\n",
            "         -5.8494e-01, -1.9444e+00, -9.7030e-01,  1.1337e+00, -2.7595e+00,\n",
            "         -2.5287e-01, -1.9908e-01, -2.8058e-01, -4.8862e-01,  7.7166e-01,\n",
            "         -2.3242e-01,  7.7208e-01,  1.5249e+00, -1.4738e-01, -1.8399e+00,\n",
            "          8.6413e-01,  1.7896e+00, -1.6958e+00,  1.0212e+00, -2.4473e-01,\n",
            "          1.3119e-01, -1.1344e+00,  7.0257e-01],\n",
            "        [-6.2924e-01, -4.4453e-01, -3.8261e-01,  4.4727e-01, -8.1995e-01,\n",
            "         -1.6368e-01, -2.2311e+00,  1.3672e+00,  9.8449e-01,  3.4585e-01,\n",
            "          2.1051e+00, -1.1523e+00,  3.2791e-01, -6.4444e-01,  5.4363e-01,\n",
            "          1.3264e+00,  7.5351e-01, -9.9126e-01, -1.9192e+00, -1.7376e-01,\n",
            "         -9.4634e-01, -2.3145e-01,  1.3509e+00,  7.5299e-01, -4.3097e-01,\n",
            "          2.1395e-01, -6.5690e-02, -4.7590e-01,  9.2738e-01, -8.3572e-01,\n",
            "          3.3385e-01,  3.5238e-01, -1.3029e-02, -7.5643e-01,  1.7131e-01,\n",
            "         -4.3951e-01,  8.3490e-01, -1.3039e-01, -1.9857e-01,  1.0063e+00,\n",
            "         -2.1510e-01,  3.4517e-01, -1.0583e+00,  2.4845e+00,  1.1930e+00,\n",
            "          2.0847e-01, -1.4161e+00,  8.5348e-01, -7.2199e-01,  8.0351e-01,\n",
            "          3.6504e-01,  1.4711e+00,  2.7204e-01,  1.6671e-01, -2.7909e+00,\n",
            "         -5.3772e-02,  1.5207e-01,  1.5348e+00,  1.5501e+00,  8.8772e-01,\n",
            "         -3.3395e-01, -6.9093e-01,  6.8450e-01,  1.9858e+00, -1.2716e+00,\n",
            "         -9.5362e-01, -3.1902e-01,  7.1138e-01,  1.0961e+00, -1.0490e+00,\n",
            "          8.3274e-01, -1.5142e+00, -2.3294e+00, -2.0047e+00,  5.6113e-02,\n",
            "         -8.5964e-01, -7.5082e-01,  1.6014e-01, -9.3284e-01,  8.0777e-01,\n",
            "         -2.8333e-01, -6.7011e-01,  1.0103e+00, -2.2016e+00,  6.3787e-01,\n",
            "         -3.1730e-01,  1.7624e+00,  1.0979e+00, -1.1574e+00,  1.6287e+00,\n",
            "          1.4559e-01,  1.3998e-01, -9.1663e-02, -1.7805e+00,  2.5252e-01,\n",
            "         -2.0196e+00, -6.3364e-01,  2.1895e+00, -2.5954e-01, -1.7108e+00,\n",
            "          7.1352e-01, -4.5379e-01,  3.3952e-01, -4.6224e-01,  1.9645e-01,\n",
            "          8.7337e-01, -1.1614e+00, -1.0533e+00, -3.0289e-01, -3.5323e-01,\n",
            "         -3.9772e-01, -2.5253e-01, -1.8840e-01,  1.5260e-01,  1.3617e+00,\n",
            "          1.6822e-01,  2.1276e+00, -6.7550e-01,  6.2388e-01, -3.7205e-01,\n",
            "          1.1231e+00,  6.4057e-01, -7.2465e-01, -5.3937e-01, -1.4702e+00,\n",
            "         -4.4526e-02, -1.9154e-01, -1.0185e+00],\n",
            "        [ 1.1158e+00, -1.2308e-01,  2.8743e-01,  6.8782e-01,  1.8805e-01,\n",
            "          1.6891e-01,  1.5715e-01, -1.1029e+00, -9.2760e-01, -3.5938e-01,\n",
            "          1.0531e+00,  2.2630e+00, -6.4215e-01,  6.1244e-01, -2.5949e-01,\n",
            "         -1.3930e+00,  4.4087e-01, -5.7893e-01,  1.8822e+00, -3.9178e-01,\n",
            "          8.7434e-01, -8.1427e-02, -1.3202e+00,  1.1828e+00,  5.4413e-03,\n",
            "         -1.1852e+00,  1.4382e+00,  7.4852e-01, -1.8048e+00,  9.8701e-01,\n",
            "         -9.3341e-01,  1.0374e+00,  2.2247e-02,  1.1544e+00, -1.1022e+00,\n",
            "         -9.3840e-04, -1.8701e+00, -2.7920e-01, -7.5671e-01,  1.5747e-01,\n",
            "         -1.5115e+00,  1.5745e+00, -5.8235e-01, -1.3619e+00, -8.9655e-01,\n",
            "          3.7417e-01, -1.0130e+00,  2.7979e-01,  4.4516e-01,  9.0500e-04,\n",
            "          7.5994e-01, -1.0516e-01, -3.1699e-02,  3.6915e-01, -3.4250e-01,\n",
            "          9.7560e-01, -6.0267e-02,  1.4042e+00, -1.8713e+00, -3.0833e-01,\n",
            "          3.9266e-01,  9.3234e-01,  1.5573e+00,  2.0246e+00,  2.4933e+00,\n",
            "         -2.3471e-01,  1.0247e+00, -5.1660e-01, -5.0690e-01,  5.6686e-01,\n",
            "          2.3874e-01, -1.1049e+00,  1.9036e-02, -8.0940e-01,  2.9853e-01,\n",
            "         -9.8047e-01,  9.6176e-01, -5.8537e-01, -1.5197e+00,  1.2861e+00,\n",
            "         -6.0523e-01,  1.7369e+00, -1.7851e+00, -6.1272e-02, -3.5771e-02,\n",
            "          5.7771e-01, -4.1013e-01, -1.3350e-01, -8.0811e-02,  1.7988e+00,\n",
            "         -7.0621e-01, -1.8562e+00, -9.4538e-01,  1.8921e-01, -2.1114e+00,\n",
            "         -6.1333e-01, -1.9837e+00,  1.5995e-01, -6.1061e-01,  2.0002e-02,\n",
            "         -1.8935e-01, -2.5732e+00, -8.2794e-01, -1.3369e+00,  6.8908e-01,\n",
            "         -1.0126e+00,  1.3416e+00,  7.2515e-01,  1.2473e+00,  4.4739e-01,\n",
            "         -3.3331e-01, -1.1313e+00,  3.5152e-02,  1.0759e+00, -1.9581e+00,\n",
            "          9.2252e-01, -5.9886e-01,  7.2895e-01, -3.4879e-01, -1.0134e+00,\n",
            "          1.0186e+00, -1.4008e+00,  1.0821e-01,  1.0081e+00, -1.9561e+00,\n",
            "         -9.9793e-01,  1.0145e+00,  2.7083e+00]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# verificar cómo los tokens son transformados por la capa de embedding\n",
        "embedded = model.embedding(input_ids)\n",
        "print(\"Sample Embedding Shape:\", embedded.shape)\n",
        "print(\"Sample Embedding Values:\", embedded[0, :5])  # Primeros 5 tokens de la primera muestra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention Output Shape: torch.Size([50, 32, 128])\n",
            "Sample Attention Output: tensor([[-0.0665, -0.0585,  0.1253, -0.0762, -0.0421],\n",
            "        [-0.0610, -0.0771,  0.0843,  0.0073, -0.0215],\n",
            "        [ 0.0152, -0.0755,  0.0952, -0.0484, -0.0589],\n",
            "        [-0.0676, -0.0711,  0.0633, -0.0788, -0.0433],\n",
            "        [ 0.0078, -0.0638,  0.0884, -0.1008,  0.0103],\n",
            "        [-0.0456, -0.0716,  0.0827, -0.0506, -0.0726],\n",
            "        [ 0.0082, -0.0847,  0.0376, -0.0934, -0.0182],\n",
            "        [ 0.0552, -0.0753,  0.0587, -0.0423, -0.0822],\n",
            "        [-0.0323, -0.1294,  0.1168, -0.0376, -0.0133],\n",
            "        [ 0.0132, -0.0809,  0.0042, -0.0133, -0.0760],\n",
            "        [-0.0636, -0.1057,  0.0407, -0.0898, -0.0819],\n",
            "        [-0.1566, -0.1415,  0.1026, -0.0461, -0.0417],\n",
            "        [-0.0842, -0.1320,  0.0410, -0.0586, -0.1178],\n",
            "        [-0.0373, -0.0883,  0.0274, -0.0494, -0.1076],\n",
            "        [-0.0777, -0.1117,  0.0767, -0.0601, -0.0662],\n",
            "        [ 0.0639, -0.1075,  0.1280, -0.0461,  0.0429],\n",
            "        [-0.0687, -0.0609,  0.1540, -0.0870, -0.0539],\n",
            "        [-0.0768, -0.0955,  0.0557, -0.1286, -0.1033],\n",
            "        [-0.0535, -0.0863,  0.1004, -0.0204, -0.0924],\n",
            "        [-0.0384, -0.0764,  0.0577, -0.0875, -0.0126],\n",
            "        [-0.0247, -0.0785,  0.0822, -0.0986, -0.0368],\n",
            "        [ 0.0237, -0.0907,  0.0926, -0.1973, -0.0363],\n",
            "        [-0.0729, -0.0912,  0.1384, -0.1342, -0.0557],\n",
            "        [ 0.0526, -0.0663,  0.0501, -0.1044, -0.0876],\n",
            "        [-0.0423, -0.0734,  0.1057, -0.0964, -0.0488],\n",
            "        [-0.0699, -0.1046,  0.1163, -0.0707, -0.1124],\n",
            "        [-0.0646, -0.1195,  0.0285, -0.0639, -0.0151],\n",
            "        [-0.1202, -0.0739,  0.0329, -0.0860, -0.0491],\n",
            "        [-0.1029, -0.0801,  0.0939, -0.0175, -0.0486],\n",
            "        [-0.0413, -0.0856,  0.0603, -0.1064, -0.0378],\n",
            "        [-0.1504, -0.1300,  0.1001, -0.0603, -0.0845],\n",
            "        [ 0.0526, -0.0663,  0.0501, -0.1044, -0.0876],\n",
            "        [ 0.0080, -0.0260,  0.1000, -0.0170, -0.0321],\n",
            "        [-0.0089, -0.0631,  0.0556, -0.0023, -0.0351],\n",
            "        [ 0.0126, -0.0314,  0.0665, -0.0223, -0.0129],\n",
            "        [ 0.0021, -0.0532,  0.0378, -0.0854, -0.1129],\n",
            "        [-0.0172, -0.0907,  0.0929, -0.0949, -0.0650],\n",
            "        [ 0.0071, -0.0835,  0.0920, -0.1079, -0.0247],\n",
            "        [-0.0065, -0.1260,  0.0377, -0.1699, -0.0091],\n",
            "        [-0.0323, -0.1026,  0.0240, -0.0567, -0.0557],\n",
            "        [-0.1161, -0.0558,  0.0819, -0.1062, -0.0512],\n",
            "        [-0.0253, -0.0953,  0.0563, -0.1608, -0.1211],\n",
            "        [-0.0874, -0.1315,  0.1025, -0.0339, -0.1135],\n",
            "        [-0.0679, -0.0829,  0.0610, -0.0723, -0.0712],\n",
            "        [-0.0314, -0.1472,  0.1055, -0.0821, -0.0545],\n",
            "        [-0.0175, -0.0821,  0.0551, -0.0657, -0.0327],\n",
            "        [-0.1622, -0.1355,  0.1125, -0.0482, -0.0668],\n",
            "        [-0.0881, -0.0334,  0.0472, -0.0674, -0.1334],\n",
            "        [-0.1007, -0.0858,  0.0796, -0.1093, -0.0388],\n",
            "        [-0.0998, -0.0856,  0.0661, -0.0967, -0.0325]],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# salida de Multi-Head Attention\n",
        "attn_output, _ = model.multihead_attn(embedded.permute(1, 0, 2), \n",
        "                                      embedded.permute(1, 0, 2), \n",
        "                                      embedded.permute(1, 0, 2))\n",
        "print(\"Attention Output Shape:\", attn_output.shape)\n",
        "print(\"Sample Attention Output:\", attn_output[:, 0, :5])  # Primer token de cada batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Layer Weights: torch.Size([1, 128])\n",
            "Final Layer Bias: Parameter containing:\n",
            "tensor([-0.0249], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "#pesos del clasificador final y ver su distribución\n",
        "print(\"Final Layer Weights:\", model.fc.weight.shape)\n",
        "print(\"Final Layer Bias:\", model.fc.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados obtenidos muestran la configuración de la capa final del modelo, la cual es una capa lineal de tamaño (1, 128), indicando que cada una de las 128 dimensiones del embedding contribuye a la salida del modelo mediante un peso específico. Estos pesos son ajustados durante el entrenamiento para optimizar la clasificación de los datos. Adicionalmente, el sesgo de la capa tiene un valor inicial de -0.0249, lo que sugiere un pequeño desplazamiento en la función de decisión antes de aplicar la activación sigmoide. Este sesgo permite mejorar la capacidad del modelo para generalizar al ajustar la frontera de decisión. Estos parámetros confirman que el modelo está correctamente estructurado para realizar clasificación binaria a partir de representaciones densas de las secuencias de entrada."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02524b7dd9304c51b5d0895da261817d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a1af3151f69403989197f7091432aef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c5ee534f9cd4fa29d5c24cca67b189d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d901d85b12f4cfb97680df26dd8a4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f4da76254b54d2a867536e5fe94f0f2",
              "IPY_MODEL_7a9afdff75834370b61278b9553e236d",
              "IPY_MODEL_269e7541a51f4a1384694d4d909c73c3"
            ],
            "layout": "IPY_MODEL_b043759f01904768b6aeb3bfa57576a2"
          }
        },
        "0dc5ff69923d4821a9fbe56425669f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "179a570fbfc2422fa1856f77b38f41b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb5546c03ccc4a7ba1aec4d8ef0035b3",
              "IPY_MODEL_783ff803cb454ea38ebd5f729862e873",
              "IPY_MODEL_5fb8c078c70a42168a86f0cba7ccd11f"
            ],
            "layout": "IPY_MODEL_02524b7dd9304c51b5d0895da261817d"
          }
        },
        "1d702235597f4140ae15907b108fc16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "269e7541a51f4a1384694d4d909c73c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5ee534f9cd4fa29d5c24cca67b189d",
            "placeholder": "​",
            "style": "IPY_MODEL_808b802749254776ae3a1c6a3271283c",
            "value": " 120000/120000 [00:22&lt;00:00, 6300.35 examples/s]"
          }
        },
        "29e82579ccf447a8bb2bd10969512c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3940058740b84b2c93b43d6b59e52d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fb8c078c70a42168a86f0cba7ccd11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a1af3151f69403989197f7091432aef",
            "placeholder": "​",
            "style": "IPY_MODEL_0dc5ff69923d4821a9fbe56425669f30",
            "value": " 7600/7600 [00:01&lt;00:00, 6779.96 examples/s]"
          }
        },
        "6f4da76254b54d2a867536e5fe94f0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8367d3f923d4a28b3cab59e4a232ef9",
            "placeholder": "​",
            "style": "IPY_MODEL_1d702235597f4140ae15907b108fc16b",
            "value": "Map: 100%"
          }
        },
        "783ff803cb454ea38ebd5f729862e873": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_967059020aa34e6985a6edd21a4bd84d",
            "max": 7600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29e82579ccf447a8bb2bd10969512c16",
            "value": 7600
          }
        },
        "7a9afdff75834370b61278b9553e236d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7acd48492004dc78a811d4a0d7099d8",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ccf9a90cf604becb029f74f1c1c3b5c",
            "value": 120000
          }
        },
        "808b802749254776ae3a1c6a3271283c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c6bc2bf7b3641828801e2b5d695ed61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "967059020aa34e6985a6edd21a4bd84d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ccf9a90cf604becb029f74f1c1c3b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b043759f01904768b6aeb3bfa57576a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7acd48492004dc78a811d4a0d7099d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb5546c03ccc4a7ba1aec4d8ef0035b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c6bc2bf7b3641828801e2b5d695ed61",
            "placeholder": "​",
            "style": "IPY_MODEL_3940058740b84b2c93b43d6b59e52d20",
            "value": "Map: 100%"
          }
        },
        "c8367d3f923d4a28b3cab59e4a232ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
